
<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">
    <link rel="stylesheet" type="text/css" href="/stylesheets/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="/stylesheets/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="/stylesheets/custom.css" />
    <!-- <link rel="stylesheet" type="text/css" href="/stylesheetsassets/lesson.css" /> -->



    <title>The 5th Asian Workshop on Reinforcement Learning</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>The 5th Asian Workshop on <br> Reinforcement Learning</h1>
        <h2>AWRL 2020</h2>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">

<ul>
  <li><a href="#intro">Scope and Background</a></li>
  <li><a href="#schedule">Invited Speakers</a></li>
  <li><a href="#timetable">Overall Program</a></li>
  <li><a href="#link">Live Link</a></li>
  <li><a href="#organizer">Workshop Chairs</a></li>
  <li><a href="#sponsor">Contact Person</a></li>
  <!--<li><a href="#acknowledgments">Acknowledgments</a></li>-->
</ul>
<h2 id ="intro">Scope and Background</h2>

<p>
  Reinforcement learning (RL) is an active field of research that deals with the problem of (single or multiple agents') sequential decision-making in unknown and possibly partially observable domains, whose dynamics may be deterministic, stochastic or adversarial. In the last few years, we have seen a growing interest in RL from both research communities and industries, and recent developments in exploration-exploitation, credit assignment, policy search, model learning, transfer/hierarchical/interactive learning, online/multi-task learning, planning, and representation learning are making RL more and more appealing to real-world applications, with promising results in challenging domains such as recommendation systems, computer games, financial marketing, intelligent transportation systems, healthcare and robotic control. 
  After great sucesses in the past four AWRL workshops held in Hamilton, New Zealand (2016), Seoul, Korea (2017), Beijing, China (2018, 2019), the 5th AWRL workshop focuses on both theoretical models, frameworks, algorithms and analysis of RL, as well as its practical applications in various real-life domains. The half-day workshop consists of sessions devoted to invited talks on specific topics on RL and presentations on publications in top conferences such as AAMAS, AAAI, IJCAI, KDD, ICML, NeurIPS. The ultimate goal is to bring together diverse viewpoints in the RL area in an attempt to consolidate the common ground, identify new research directions, and promote the rapid advance of RL research community.
</p>

<hr/>

<!--TODO : this schedule will need to change to more accurately reflect what we are teaching -->
<h2 id="schedule">Invited Speakers</h2>
<div id="wrapper">
      <div class="figure1">
          <figure class="thumbnail">
              <a href="http://www.jilsa.net/xinxu.html"><img src="img/xuxin5.jpg" class="img-responsive" alt="Image"  height="200"></a>
              <figcaption class="caption text-center">
                <a href="http://www.jilsa.net/xinxu.html"><p><strong>Xin Xu</a></strong><br>
                      <small>National University of Defense Technology</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure1 --> 
      <div class="figure1">
          <figure class="thumbnail">
              <a href="http://school.freekaoyan.com/bj/gscas/daoshi/2016/05-10/1462854999590425.shtml"><img src="img/dongbinzhao.jpg" class="img-responsive" alt="Image"  height="200"></a>
              <figcaption class="caption text-center">
                <a href="http://school.freekaoyan.com/bj/gscas/daoshi/2016/05-10/1462854999590425.shtml"><p><strong>Dongbin Zhao</a></strong><br>
                      <small>Chinese Academy of Sciences</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure1 --> 
      <div class="figure2">
          <figure class="thumbnail">
              <img src="img/zhaoli.jpg" class="img-responsive" alt="Image"  height="200"></a>
              <figcaption class="caption text-center">
                <p><strong>Li Zhao</a></strong><br>
                      <small>Microsoft Research Asia</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure2 --> 
</div>
<!-- /.container -->

</p>

<div id="wrapper">
      <div class="figure3">
          <figure class="thumbnail">
              <a href="https://ai.nankai.edu.cn/info/1035/3581.htm"><img src="img/xianguo.jpg" class="img-responsive" alt="Image"  height="200"></a>
              <figcaption class="caption text-center">
                <a href="https://ai.nankai.edu.cn/info/1035/3581.htm"><p><strong>Xian Guo</a></strong><br>
                      <small>Nankai University</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure3 --> 
      <div class="figure4">
          <figure class="thumbnail">
              <a href="http://people.ucas.edu.cn/~ZHANGJUNGE"><img src="img/jungezhang.jpg" class="img-responsive" alt="Image"  height="200"></a>
              <figcaption class="caption text-center">
                <a href="http://people.ucas.edu.cn/~ZHANGJUNGE"><p><strong>Junge Zhang</a></strong><br>
                      <small>Chinese Academy of Sciences</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure4 --> 
      <div class="figure4">
          <figure class="thumbnail">
              <a href="http://staff.ustc.edu.cn/~wufeng02/"><img src="img/fengwu.jpg" class="img-responsive" alt="Image"  height="200"></a>
              <figcaption class="caption text-center">
                <a href="http://staff.ustc.edu.cn/~wufeng02/"><p><strong>Feng Wu</a></strong><br>
                      <small>University of Science and Technology of China</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure5 --> 
</div>
<!-- /.container -->


<!--TODO : this schedule will need to change to more accurately reflect what we are teaching -->
<h2 id="timetable">Overall Program</h2>
    <table class="table table-striped" width="500">
      <tr><td class="aligncenter" width="25%">9:00-9:40 (GMT+8)</td> <td class="aligncenter" width="25%"> Reinforcement Learning for Optimized Decision-making and Control of Intelligent Robots</td> <td class="aligncenter" width="25%">Xin Xu</td> <td class="aligncenter" width="25%">National University of Defense Technology</td></tr>
      <tr><td class="aligncenter">9:40-10:20 (GMT+8)</td> <td class="aligncenter"> Artificial intelligence methods in real-time fighting game </td> <td class="aligncenter"> Dongbin Zhao</td> </td> <td class="aligncenter"> Chinese Academy of Sciences </td></tr>
      <tr><td class="aligncenter">10:20-10:50 (GMT+8)</td> <td class="aligncenter"> Reward Decomposition: Discover and Leverage Decomposable Structure in Deep Reinforcement Learning </td> <td class="aligncenter">Li Zhao </td> <td class="aligncenter">Microsoft Research Asia</td></tr>
      <tr><td class="aligncenter">10:50-11:20 (GMT+8)</td> <td class="aligncenter"> Path integral reinforcement learning and its application in control system </td> <td class="aligncenter">Xian Guo </td> <td class="aligncenter">Nankai University</td></tr>
      <tr><td class="aligncenter" >11:20-11:50 (GMT+8)</td> <td class="aligncenter"> Model-based Reinforcement Learning </td> <td class="aligncenter">Junge Zhang </td> <td class="aligncenter"> Chinese Academy of Sciences</td></tr>
      <tr><td class="aligncenter">11:50-12:20 (GMT+8)</td> <td class="aligncenter"> Multi-agent reinforcement learning from the perspective of model complexity </td> <td class="aligncenter">Feng Wu </td> <td class="aligncenter"> University of Science and Technology of China</td></tr>
    </table>
<hr/>

<h3>Talks</h3>
<p><strong>Topic: Reinforcement Learning for Optimized Decision-making and Control of Intelligent Robots</strong><br>
Xin Xu, National University of Defense Technology<br>
<strong>Time:</strong> 9:00-9:40 (GMT+8)<br>
<strong>Abstract:</strong> This talk will analyze the technical requirements and research challenges of intelligent robots under complex environments. To deal with the above challenges, the major models and algorithm frameworks of reinforcement learning (RL) will be introduced, together with some recent advances of feature representation and receding-horizon policy optimization in reinforcement learning algorithms.Then, some applications of RL in autonomous control and human-machine cooperative driving of intelligent vehicles will be introduced.Finally, the future research directions in related areas will also be discussed.   <br><br>

<p><strong>Topic: Artificial intelligence methods in real-time fighting games</strong><br>
Dongbin Zhao, Chinese Academy of Sciences<br>
<strong>Time:</strong> 9:40-10:20 (GMT+8)<br>
<strong>Abstract:</strong> Real-time fighting game is a typical one-to-one character confrontation game, to win the opponent during the limited time by effectively hit, which is an important research direction in the field of game artificial intelligence (AI). In recent years, AI methods represented by deep reinforcement learning and statistical forward planning have made breakthroughs in games. This talk will briefly introduce the fighting game, together with the mainly used artificial intelligence methods, analyzes their advantages and disadvantages. I will focus on the proposed method combining statistical forward planning and enhanced opponent modeling with reinforcement learning (the champion of fighting game AI competition in 2020 Conference on Games), and draft some future research trends of related fields.   <br><br>

<p><strong>Topic: Reward Decomposition: Discover and Leverage Decomposable Structure in Deep Reinforcement Learning</strong><br>
Li Zhao, Microsoft Research Asia<br>
<strong>Time:</strong> 10:20-10:50 (GMT+8)<br>
<strong>Abstract:</strong> In many environments the reward can be decomposed into sub-rewards obtained from different sources. Such decomposition can be further leveraged to improve sample efficiency of DRL algorithms and/or to gain better interpretability. Most existing works on reward decomposition require prior knowledge(such as decomposed state) to learn decomposed reward. We propose novel reward decomposition methods that can decompose reward without prior knowledge as well as improve sample efficiency of DRL algorithms.  <br><br>

<p><strong>Topic: Path integral reinforcement learning and its application in control system</strong><br>
Xian Guo, Nankai University<br>
<strong>Time:</strong> 10:50-11:20 (GMT+8)<br>
<strong>Abstract:</strong> In recent years, reinforcement learning technology has made significant breakthroughs in many fields. However, reinforcement learning techniques based on Markov decision process often perform well only in discrete action situations, and are unstable in continuous control systems based on differential equations. This report introduces reinforcement learning algorithms based on path integral. The theoretical basis of the algorithms is Hamilton Jaccobi Bellman equation. For this equation, by introducing appropriate assumptions, the statistical solution is obtained. The solution of the differential equation is transformed into solving the mathematical expectation. Finally, the approximate estimation of the expectation and the current iterative optimal strategy are obtained. This report first introduces the basic principle of path integral reinforcement learning algorithms, and then introduces two specific applications of the method in the control system: (1) aiming at the problem of robot path tracking control, combined with the traditional nonlinear control algorithm, we propose a better intelligent path tracking method than the traditional control method; (2) for the attitude control problem of high-speed aircraft based on the constraints of attitude control problem and PID controller, an efficient, robust and safe attitude control algorithm is proposed. In view of these two specific control problems, we share the research ideas and put forward the research prospects of reinforcement learning in the field of control. <br><br>

<p><strong>Topic: Model-based Reinforcement Learning</strong><br>
Junge Zhang, Chinese Academy of Sciences<br>
<strong>Time:</strong> 11:20-11:50 (GMT+8)<br>
<strong>Abstract:</strong> DeepMindâ€™s AlphaX (AlphaGo, AlphaZero, AlphaStar) have obtained great success in Go, Starcraft and all these systems consume huge computation resources. Few organizations can afford such huge cost to train an agent. Thus, data inefficiency becomes a major drawback for such advanced techniques. Model based reinforcement learning tries to capture the dynamics to greatly improve the data efficiency and is regarded as a promising framework to address such a limitation. This report will introduce the latest progress of model-based reinforcement learning and the three dilemmas faced by MBRL. Then we will discuss our work in this line of research. Finally, this report will summarize some possible research directions in the future. <br><br>

<p><strong>Topic: Multi-agent reinforcement learning from the perspective of model complexity</strong><br>
Feng Wu, University of Science and Technology of China<br>
<strong>Time:</strong> 11:50-12:20 (GMT+8)<br>
<strong>Abstract:</strong> In recent years, multi-agent reinforcement learning has made a lot of important progress, but it still faces great challenges when applied to real problems. Part of this comes from the complexity of the reinforcement learning method itself, and the other part comes from the difficulty of multi-agent distributed decision-making. For example, the complexity of solving the MDP model of a single agent is P, while the complexity of the Dec-POMDP model of multiple agents is NEXP (much greater than P). The huge difference in model complexity determines that multi-agent distributed decision-making is fundamentally much more difficult than single-agent decision-making, and reinforcement learning as a learning-based solution method cannot be spared. At present, most enhanced algorithms have more or less implicit preconditions when applied to multi-agent decision-making. These conditions may weaken the complexity of the problem to a certain extent. When real problems do not meet these (implicit) conditions, these algorithms are usually difficult to perform satisfactorily. This report will discuss the design of multi-agent reinforcement learning algorithms from the perspective of model complexity, in the hope of benefiting the practical application of multi-agent reinforcement learning algorithms.<br>


<h2>Live Link</h2>
<strong>DAI 2020:</strong> <a href="https://dai2020.163.com/m/#live">https://dai2020.163.com/m/#live</a><br>


<h2 id="organizer">Workshop Chairs</h2>

<div id="wrapper">
      <div class="figure1">
          <figure class="thumbnail">
              <a href="http://sdcs.sysu.edu.cn/content/4883"><img src="img/chaoyu.jpg" class="img-responsive" alt="Image"  height="200"></a>
              <figcaption class="caption text-center">
                <a href="http://sdcs.sysu.edu.cn/content/4883"><p><strong>Chao Yu</a></strong><br>
                      <small>Sun Yat-sen University</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure1 --> 
  
      <div class="figure2">
          <figure class="thumbnail">
              <a href="http://www.lamda.nju.edu.cn/yuy/"><img src="img/yangyu.jpg" class="img-responsive" alt="Image" width="200" height="200"></a>
              <figcaption class="caption text-center">
                <a href="http://www.lamda.nju.edu.cn/yuy/"><p><strong>Yang Yu</a></strong><br>
                      <small>Nanjing University</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure2 -->
      <div class="figure3">
          <figure class="thumbnail">
              <a href="https://ai.nju.edu.cn/zhangzongzhang/"><img src="img/zzz.jpg" class="img-responsive" alt="Image" width="200" height="200"></a>
              <figcaption class="caption text-center">
                <a href="https://ai.nju.edu.cn/zhangzongzhang/"><p><strong>ZongZhang Zhang</a></strong><br>
                      <small>Nanjing University</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure3 -->

</div>
<!-- /.container -->

<h2 id="sponsor">Contact Person</h2>
<p>
    Dr. Chao Yu<br>
    Sun Yat-sen University, China<br>
    <strong>Email:</strong> yuchao3@mail.sysu.edu.cn<br>

</p>



<!--
</div>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>
  This workshop was made possible by professor Weinan Zhang.
</p>
-->

<!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=dQXgc8uBIK8rvujrNmLXRN3tKPsfMmPJUs36IZRUM7w&cl=ffffff&w=a"></script>-->
<br><br><br><br><br><br><br><br><br><br><br><br><br><br>


<br>
<p>
  This page was generated by <a href="https://pages.github.com">GitHub
  Pages</a> using the Architect theme
  by <a href="https://twitter.com/jasonlong">Jason Long</a>.
</p>

      </div>
    </div>
  </body>
</html>
