
<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">
    <link rel="stylesheet" type="text/css" href="/stylesheets/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="/stylesheets/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="/stylesheets/custom.css" />
    <!-- <link rel="stylesheet" type="text/css" href="/stylesheetsassets/lesson.css" /> -->



    <title>The 5th Asian Workshop on Reinforcement Learning</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>The 5th Asian Workshop on <br> Reinforcement Learning</h1>
        <h2>AWRL 2020</h2>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">

<ul>
  <li><a href="#intro">Scope and Background</a></li>
  <li><a href="#schedule">Invited Speakers</a></li>
  <li><a href="#organizer">Workshop Chairs</a></li>
  <li><a href="#timetable">Overall Program</a></li>
  <li><a href="#sponsor">Contact Person</a></li>
  <!--<li><a href="#acknowledgments">Acknowledgments</a></li>-->
</ul>
<h2 id ="intro">Scope and Background</h2>

<p>
  Reinforcement learning (RL) is an active field of research that deals with the problem of (single or multiple agents') sequential decision-making in unknown and possibly partially observable domains, whose dynamics may be deterministic, stochastic or adversarial. In the last few years, we have seen a growing interest in RL from both research communities and industries, and recent developments in exploration-exploitation, credit assignment, policy search, model learning, transfer/hierarchical/interactive learning, online/multi-task learning, planning, and representation learning are making RL more and more appealing to real-world applications, with promising results in challenging domains such as recommendation systems, computer games, financial marketing, intelligent transportation systems, healthcare and robotic control. 
  After great sucesses in the past four AWRL workshops held in Hamilton, New Zealand (2016), Seoul, Korea (2017), Beijing, China (2018, 2019), the 5th AWRL workshop focuses on both theoretical models, frameworks, algorithms and analysis of RL, as well as its practical applications in various real-life domains. The half-day workshop consists of sessions devoted to invited talks on specific topics on RL and presentations on publications in top conferences such as AAMAS, AAAI, IJCAI, KDD, ICML, NeurIPS. The ultimate goal is to bring together diverse viewpoints in the RL area in an attempt to consolidate the common ground, identify new research directions, and promote the rapid advance of RL research community.
</p>

<hr/>

<!--TODO : this schedule will need to change to more accurately reflect what we are teaching -->
<h2 id="schedule">Invited Speakers</h2>
<div id="wrapper">
      <div class="figure1">
          <figure class="thumbnail">
              <a href="http://www.jilsa.net/xinxu.html"><img src="img/xuxin5.jpg" class="img-responsive" alt="Image"  height="200"></a>
              <figcaption class="caption text-center">
                <a href="http://www.jilsa.net/xinxu.html"><p><strong>Xin Xu</a></strong><br>
                      <small>National University of Defense Technology</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure1 --> 
      <div class="figure1">
          <figure class="thumbnail">
              <a href="http://school.freekaoyan.com/bj/gscas/daoshi/2016/05-10/1462854999590425.shtml"><img src="img/dongbinzhao.jpg" class="img-responsive" alt="Image"  height="200"></a>
              <figcaption class="caption text-center">
                <a href="http://school.freekaoyan.com/bj/gscas/daoshi/2016/05-10/1462854999590425.shtml"><p><strong>Dongbin Zhao</a></strong><br>
                      <small>Chinese Academy of Sciences</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure1 --> 
      <div class="figure2">
          <figure class="thumbnail">
              <img src="img/zhaoli.jpg" class="img-responsive" alt="Image"  height="200"></a>
              <figcaption class="caption text-center">
                <p><strong>Li Zhao</a></strong><br>
                      <small>Microsoft Research Asia</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure2 --> 
</div>
<!-- /.container -->

</p>

<div id="wrapper">
      <div class="figure3">
          <figure class="thumbnail">
              <a href="https://ai.nankai.edu.cn/info/1035/3581.htm"><img src="img/xianguo.jpg" class="img-responsive" alt="Image"  height="200"></a>
              <figcaption class="caption text-center">
                <a href="https://ai.nankai.edu.cn/info/1035/3581.htm"><p><strong>Xian Guo</a></strong><br>
                      <small>Nankai University</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure3 --> 
      <div class="figure4">
          <figure class="thumbnail">
              <a href="http://people.ucas.edu.cn/~ZHANGJUNGE"><img src="img/jungezhang.jpg" class="img-responsive" alt="Image"  height="200"></a>
              <figcaption class="caption text-center">
                <a href="http://people.ucas.edu.cn/~ZHANGJUNGE"><p><strong>Junge Zhang</a></strong><br>
                      <small>Chinese Academy of Sciences</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure4 --> 
      <div class="figure4">
          <figure class="thumbnail">
              <a href="http://staff.ustc.edu.cn/~wufeng02/"><img src="img/fengwu.jpg" class="img-responsive" alt="Image"  height="200"></a>
              <figcaption class="caption text-center">
                <a href="http://staff.ustc.edu.cn/~wufeng02/"><p><strong>Feng Wu</a></strong><br>
                      <small>University of Science and Technology of China</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure5 --> 
</div>
<!-- /.container -->

<h2 id="organizer">Workshop Chairs</h2>

<div id="wrapper">
      <div class="figure1">
          <figure class="thumbnail">
              <a href="http://sdcs.sysu.edu.cn/content/4883"><img src="img/chaoyu.jpg" class="img-responsive" alt="Image"  height="200"></a>
              <figcaption class="caption text-center">
                <a href="http://sdcs.sysu.edu.cn/content/4883"><p><strong>Chao Yu</a></strong><br>
                      <small>Sun Yat-sen University</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure1 --> 
  
      <div class="figure2">
          <figure class="thumbnail">
              <a href="http://www.lamda.nju.edu.cn/yuy/"><img src="img/yangyu.jpg" class="img-responsive" alt="Image" width="200" height="200"></a>
              <figcaption class="caption text-center">
                <a href="http://www.lamda.nju.edu.cn/yuy/"><p><strong>Yang Yu</a></strong><br>
                      <small>Nanjing University</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure2 -->
      <div class="figure3">
          <figure class="thumbnail">
              <a href="https://ai.nju.edu.cn/zhangzongzhang/"><img src="img/zzz.jpg" class="img-responsive" alt="Image" width="200" height="200"></a>
              <figcaption class="caption text-center">
                <a href="https://ai.nju.edu.cn/zhangzongzhang/"><p><strong>ZongChang Zhang</a></strong><br>
                      <small>Nanjing University</small>
                </p>
              </figcaption>
          </figure>
      </div>
      <!-- figure3 -->

</div>
<!-- /.container -->


<!--TODO : this schedule will need to change to more accurately reflect what we are teaching -->
<h2 id="timetable">Overall Program</h2>
    <table class="table table-striped" width="500">
      <tr><td class="aligncenter" width="25%">9:00-9:40 (GMT+8)</td> <td class="aligncenter" width="25%"> Reinforcement Learning for Optimized Decision-making and Control of Intelligent Robots</td> <td class="aligncenter" width="25%">Xin Xu</td> <td class="aligncenter" width="25%">National University of Defense Technology</td></tr>
      <tr><td class="aligncenter">9:40-10:20 (GMT+8)</td> <td class="aligncenter"> Artificial intelligence methods in real-time fighting game </td> <td class="aligncenter"> Dongbin Zhao</td> </td> <td class="aligncenter"> Chinese Academy of Sciences </td></tr>
      <tr><td class="aligncenter">10:20-10:50 (GMT+8)</td> <td class="aligncenter"> Reward Decomposition: Discover and Leverage Decomposable Structure in Deep Reinforcement Learning </td> <td class="aligncenter">Li Zhao </td> <td class="aligncenter">Microsoft Research Asia</td></tr>
      <tr><td class="aligncenter">10:50-11:20 (GMT+8)</td> <td class="aligncenter"> Path integral reinforcement learning and its application in control system </td> <td class="aligncenter">Xian Guo </td> <td class="aligncenter">Nankai University</td></tr>
      <tr><td class="aligncenter" >11:20-11:50 (GMT+8)</td> <td class="aligncenter"> Model-based Reinforcement Learning </td> <td class="aligncenter">Junge Zhang </td> <td class="aligncenter"> Chinese Academy of Sciences</td></tr>
      <tr><td class="aligncenter">11:50-12:20 (GMT+8)</td> <td class="aligncenter"> Multi-agent reinforcement learning from the perspective of model complexity </td> <td class="aligncenter">Feng Wu </td> <td class="aligncenter"> University of Science and Technology of China</td></tr>
    </table>
<hr/>

<h3>Talks</h3>
<p><strong>Title: Reinforcement Learning for Optimized Decision-making and Control of Intelligent Robots</strong><br>
Xin Xu, National University of Defense Technology<br>
<strong>Time:</strong> 9:00-9:40 (GMT+8)<br>
<strong>Abstract:</strong> This talk will analyze the technical requirements and research challenges of intelligent robots under complex environments. To deal with the above challenges, the major models and algorithm frameworks of reinforcement learning (RL) will be introduced, together with some recent advances of feature representation and receding-horizon policy optimization in reinforcement learning algorithms.Then, some applications of RL in autonomous control and human-machine cooperative driving of intelligent vehicles will be introduced.Finally, the future research directions in related areas will also be discussed.   <br>

<p><strong>Title: Artificial intelligence methods in real-time fighting games</strong><br>
Dongbin Zhao, Chinese Academy of Sciences<br>
<strong>Time:</strong> 9:40-10:20 (GMT+8)<br>
<strong>Abstract:</strong> Real-time fighting game is a typical one-to-one character confrontation game, to win the opponent during the limited time by effectively hit, which is an important research direction in the field of game artificial intelligence (AI). In recent years, AI methods represented by deep reinforcement learning and statistical forward planning have made breakthroughs in games. This talk will briefly introduce the fighting game, together with the mainly used artificial intelligence methods, analyzes their advantages and disadvantages. I will focus on the proposed method combining statistical forward planning and enhanced opponent modeling with reinforcement learning (the champion of fighting game AI competition in 2020 Conference on Games), and draft some future research trends of related fields.   <br>

<p><strong>Title: Reward Decomposition: Discover and Leverage Decomposable Structure in Deep Reinforcement Learning</strong><br>
Li Zhao, Microsoft Research Asia<br>
<strong>Time:</strong> 10:20-10:50 (GMT+8)<br>
<strong>Abstract:</strong> In many environments the reward can be decomposed into sub-rewards obtained from different sources. Such decomposition can be further leveraged to improve sample efficiency of DRL algorithms and/or to gain better interpretability. Most existing works on reward decomposition require prior knowledge(such as decomposed state) to learn decomposed reward. We propose novel reward decomposition methods that can decompose reward without prior knowledge as well as improve sample efficiency of DRL algorithms.  <br>

<p><strong>Title: Path integral reinforcement learning and its application in control system</strong><br>
Xian Guo, Nankai University<br>
<strong>Time:</strong> 10:50-11:20 (GMT+8)<br>
<strong>Abstract:</strong> 近年来，强化学习技术在越来越多的领域取得突破性进展。然而，以马尔科夫决策过程为理论框架的强化学习技术往往只在动作离散的情景中表现很好，在动作连续的以微分方程为驱动的控制系统中表现不稳定。本报告介绍基于路径积分的强化学习算法，该算法的理论基础是哈密尔顿-雅克比-贝尔曼方程，针对该方程，通过引入恰当的假设，得到统计解，将微分方程的解转化为求解数学期望，最终用数据得到该期望的近似估计及当前的迭代最优策略。本报告首先介绍路径积分强化学习算法的基本原理，然后介绍课题组基于该方法在控制系统中的具体两个应用：（1）针对机器人路径跟踪控制问题，结合传统的非线性控制算法，实现比传统控制方法更好的智能路径跟踪方法；（2）针对高速飞行器的姿态控制问题，结合姿态控制问题本身的约束、pid控制器，提出高效鲁棒安全的姿态控制算法。针对这两个具体的控制问题，分享研究思路并提出强化学习在控制领域中的研究展望。 <br>

<p><strong>Title: Model-based Reinforcement Learning</strong><br>
Junge Zhang, Chinese Academy of Sciences<br>
<strong>Time:</strong> 11:20-11:50 (GMT+8)<br>
<strong>Abstract:</strong> DeepMind’s AlphaX (AlphaGo, AlphaZero, AlphaStar) have obtained great success in Go, Starcraft and all these systems consume huge computation resources. Few organizations can afford such huge cost to train an agent. Thus, data inefficiency becomes a major drawback for such advanced techniques. Model based reinforcement learning tries to capture the dynamics to greatly improve the data efficiency and is regarded as a promising framework to address such a limitation. This report will introduce the latest progress of model-based reinforcement learning and the three dilemmas faced by MBRL. Then we will discuss our work in this line of research. Finally, this report will summarize some possible research directions in the future. <br>

<p><strong>Title: Multi-agent reinforcement learning from the perspective of model complexity</strong><br>
Feng Wu, University of Science and Technology of China<br>
<strong>Time:</strong> 11:50-12:20 (GMT+8)<br>
<strong>Abstract:</strong> 近年来，多智能体（深度）强化学习取得了大量重要的进展，但应用到现实问题中依然面临较大的挑战。这其中一部分来源于强化学习方法本身的复杂性，另一部分则源自多智能体分布式决策所带来的难度。例如单个智能体的MDP模型的求解复杂度是P，而多个智能体的Dec-POMDP模型的复杂度是NEXP（远大于P）。模型复杂度上的巨大差异，决定了多智能体分布式决策从根本上比单智能体的决策要难得多，强化学习作为一种基于学习的求解方法也无法幸免。当前，大多数强化算法在应用于多智能体决策时，都或多或少的隐含了若干的前提条件。这些条件在一定程度上可能弱化了问题的复杂度。当现实问题不满足这些（隐含）条件时，这些算法通常难以有满意的表现。本次报告将从模型复杂性的角度，对多智能体强化学习算法的设计进行探讨，以期能对多智能体强化学习算法的实际应用有所裨益。<br>


<h2 id="sponsor">Contact Person</h2>
<p>
    Dr. Chao Yu<br>
    Sun Yat-sen University, China<br>
    <strong>Email:</strong> yuchao3@mail.sysu.edu.cn<br>

</p>



<!--
</div>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>
  This workshop was made possible by professor Weinan Zhang.
</p>
-->

<!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=dQXgc8uBIK8rvujrNmLXRN3tKPsfMmPJUs36IZRUM7w&cl=ffffff&w=a"></script>-->
<br><br><br><br><br><br><br><br><br><br><br><br><br><br>


<br>
<p>
  This page was generated by <a href="https://pages.github.com">GitHub
  Pages</a> using the Architect theme
  by <a href="https://twitter.com/jasonlong">Jason Long</a>.
</p>

      </div>
    </div>
  </body>
</html>
